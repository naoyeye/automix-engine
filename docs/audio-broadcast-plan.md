# automix-engine 音频广播/推流方案（局域网/多设备收听）

## 背景与目标

目标：**在本机运行 AutoMix Engine（`automix-playlist` 构建播放列表、`automix-play` 输出音频）后，让局域网内的其他电脑/手机能够收听到本机实时输出的音频**。

理想体验：其他设备打开 `http://<本机IP>:<端口>`（或等价入口）即可听到实时混音输出。

## 现状与可行性结论（基于当前代码结构）

结论：**可实现**，但需要新增“网络输出/编码/服务端分发”层。

当前项目已经具备以下关键能力：

- **实时音频渲染输出（PCM）**：引擎侧提供 `render(float* buffer, int frames)`，C API 提供 `automix_render()`（由 `automix-play` 的 CoreAudio 回调驱动，或可由调用方手动 pull）。
- **控制线程轮询**：主线程/控制线程定期 `poll()`（C API：`automix_poll()`）以驱动预加载、过渡完成、状态回调等非实时逻辑。
- **默认平台播放输出（macOS CoreAudio）**：`automix_start_audio()` 启动平台音频设备，音频回调里拉取引擎 PCM。

因此，最“干净”的实现方式是：**直接从 `automix_render()` 拿到引擎生成的 PCM，再编码并通过网络协议分发**（而不是从系统声卡抓音频）。

## 核心数据流（建议的网络推流架构）

### 基本数据流

1. 控制线程：
   - 初始化 engine / 生成 playlist / `automix_play()`
   - 每 ~20ms 调用 `automix_poll()`
2. 音频线程（或推流线程）：
   - 循环调用 `automix_render()` 拉取 PCM（立体声 float32 interleaved）
   - 编码（Opus/AAC/MP3…）
   - 封装并分发（HTTP/WebSocket/RTSP/RTP/WebRTC/HLS…）

### 关键注意点

- **时钟/节拍与采样率**：网络输出应以引擎的实际采样率为准；必要时引入重采样（`swresample` 或其他库）。
- **抖动缓冲**：网络/浏览器端需要 buffer 来抵御抖动；低延迟方案要更精细的 jitter buffer。
- **断线重连**：客户端断线后重新加入，服务端要能继续推流；对 HLS 则天然支持重新拉清单。
- **多客户端**：需要 fan-out（一个编码器输出被多个连接消费），避免每个客户端重复编码造成 CPU 浪费。

## 方案选型（按“ip:port 一键听”优先级）

下面按“用户体验（浏览器直连）”→“工程量/稳定性”→“延迟”给出推荐。

### 方案 A：HTTP 页面 + WebSocket/HTTP Chunked 推流（推荐：最贴近 `ip:port` 一键听）

**服务端（本机）**
- `automix_render()` → PCM
- 编码：**Opus**（低码率高音质，低延迟）或 **AAC**（iOS 生态友好）
- 分发：
  - 页面：HTTP 提供一个极简播放器页面
  - 音频数据：WebSocket（低延迟）或 HTTP chunked（实现较简单）

**客户端**
- 手机/电脑浏览器打开 `http://ip:port`
- JS 播放（需要实现 buffer、解码、音频时钟同步）

**优点**
- 跨平台最好（浏览器即可）
- 满足“访问 ip:port 就能听”

**缺点**
- 工程量中等：浏览器端播放链路、buffer/jitter、断线重连

适合目标：**低到中延迟（~0.3s–2s）** + **最好用的接入方式**

### 方案 B：RTSP/RTP（VLC/ffplay 等客户端友好）

**服务端（本机）**
- `automix_render()` → PCM → 编码（AAC/Opus）→ RTP/RTSP

**客户端**
- VLC/ffplay 等打开 `rtsp://ip:port/...` 或 `udp://@port`

**优点**
- 实现相对直接
- 延迟可做得较低

**缺点**
- 不一定能“浏览器原生播放”
- 移动端通常需要 VLC 或特定播放器 App

适合目标：**允许客户端装播放器 App**，追求相对低延迟

### 方案 C：HLS（HTTP .m3u8 分段播放，最稳但延迟大）

**服务端（本机）**
- `automix_render()` → 编码（AAC 常见）→ 分段（2–6s/片）→ `m3u8`

**客户端**
- iOS/Safari 原生支持，其他平台也有大量播放器支持

**优点**
- 极稳、兼容性很强
- 断线重连简单

**缺点**
- 延迟通常 **6–30 秒**，不适合“实时 DJ 监听”

适合目标：**只要能听、稳定优先，不强调实时性**

### 方案 D：WebRTC（最低延迟、体验好，但实现复杂）

**服务端（本机）**
- `automix_render()` → WebRTC 音频 track
- 需要信令、ICE（局域网可简化但仍有复杂度）

**客户端**
- 浏览器直接播放，低延迟

**优点**
- 低延迟、抗抖动能力强

**缺点**
- 工程复杂度最高（信令/ICE/STUN/TURN 等）

适合目标：**强低延迟（< 300ms）**，愿意投入较多工程成本

## “可换成蓝牙/其他方式吗？”

可以，但是否“符合多设备、ip:port 即开即听”的目标要谨慎评估。

### 蓝牙（A2DP）

- 典型用途是：电脑把音频发给 **单个**蓝牙音箱/耳机（点对点，且受系统限制）
- 若希望“手机像蓝牙音箱一样接收电脑音频”，需要手机支持 A2DP Sink：
  - iOS 通常不作为通用 Sink
  - Android 可行性更高但限制也多

结论：蓝牙更像“换输出设备”，不太适合“多设备同时收听”。

### AirPlay / Chromecast / DLNA

- 适合投到特定生态设备（音箱/电视）
- 生态依赖强，跨设备通用性不如 HTTP/WebRTC

结论：可作为补充输出通道，但不是最通用的“多设备局域网收听方案”。

### UDP Multicast（局域网组播）

- 适合“一台发、多台收”，网络与 CPU 利用率好
- 但 Wi‑Fi/路由对组播支持不一；客户端接入门槛更高

结论：若明确同一 Wi‑Fi 多设备监听且愿意做客户端，这条路性能很好。

## 实现落地建议（后续实现时可按阶段推进）

### 阶段 1：最小可用（MVP）

- 新增一个独立 CLI（示例名）：`automix-stream`
  - 复用 `automix-play` 的 playlist 生成与控制循环
  - 将音频输出从 CoreAudio 改为“网络输出”（仍可选保留本机播放）
- MVP 目标：1 个客户端能稳定收听（先不做多客户端 fan-out 也可）

### 阶段 2：多客户端与稳定性

- 引入“单编码器 + 多连接分发”（fan-out）
- 增加断线重连策略、客户端缓冲策略、基本的健康检查

### 阶段 3：低延迟优化

- 优化编码与包间隔（frame size）、端到端 buffer
- 若走 WebRTC，补全信令与连接管理

## 备注：为什么优先“直接用 automix_render()”而不是抓系统声卡

- **可控性强**：采样率、格式、音量、延迟都在进程内掌控
- **跨平台更容易**：不依赖虚拟声卡/系统权限
- **更低延迟/更稳定**：少一层系统音频环路

